import { NextRequest, NextResponse } from 'next/server'
import { getLLMService, AGRICULTURAL_SYSTEM_PROMPT, ChatMessage, FunctionCall } from '@/lib/llm'
import { prisma } from '@/lib/prisma'

// Function to execute John Deere API calls
async function executeJohnDeereFunction(functionCall: FunctionCall): Promise<any> {
  const { name, arguments: args } = functionCall
  
  console.log(`üîß Executing function: ${name}`, args)
  
  try {
    const baseUrl = process.env.NEXTAUTH_URL || 'http://localhost:3000'
    let url: string
    
    switch (name) {
      case 'getOrganizations':
        url = `${baseUrl}/api/johndeere/organizations`
        break
      case 'getFields':
        url = `${baseUrl}/api/johndeere/organizations/${args.orgId}/fields`
        break
      case 'getEquipment':
        url = `${baseUrl}/api/johndeere/organizations/${args.orgId}/equipment`
        break
      case 'getOperations':
        url = `${baseUrl}/api/johndeere/organizations/${args.orgId}/operations`
        break
      case 'getComprehensiveData':
        url = `${baseUrl}/api/johndeere/organizations/${args.orgId}/comprehensive`
        break
      default:
        throw new Error(`Unknown function: ${name}`)
    }

    console.log(`üì° Making API call to: ${url}`)
    
    const response = await fetch(url, {
      method: 'GET',
      headers: {
        'Content-Type': 'application/json',
      },
    })

    console.log(`üì° API response status: ${response.status}`)
    
    if (!response.ok) {
      const errorText = await response.text()
      console.error(`‚ùå API call failed: ${response.status} - ${errorText}`)
      throw new Error(`API call failed: ${response.status} - ${errorText}`)
    }

    const data = await response.json()
    console.log(`‚úÖ Function ${name} completed successfully`, { dataKeys: Object.keys(data) })
    
    return data
  } catch (error) {
    console.error(`‚ùå Error executing function ${name}:`, error)
    return { 
      error: `Failed to execute ${name}: ${error instanceof Error ? error.message : 'Unknown error'}`,
      functionName: name,
      arguments: args
    }
  }
}

export async function POST(request: NextRequest) {
  console.log('üöÄ Starting chat completion request')
  
  try {
    const { sessionId, messages, options } = await request.json()
    console.log('üìù Request data:', { sessionId, messageCount: messages?.length, options })

    if (!sessionId || !messages || !Array.isArray(messages)) {
      console.error('‚ùå Missing required fields')
      return NextResponse.json(
        { error: 'Missing required fields: sessionId, messages' },
        { status: 400 }
      )
    }

    // TODO: Get user ID from authentication session
    const userId = 'user_placeholder'

    // Verify session belongs to user
    console.log('üîç Verifying session:', sessionId)
    const session = await prisma.chatSession.findFirst({
      where: {
        id: sessionId,
        userId: userId,
      },
    })

    if (!session) {
      console.error('‚ùå Chat session not found:', sessionId)
      return NextResponse.json(
        { error: 'Chat session not found' },
        { status: 404 }
      )
    }

    // Get LLM service
    console.log('ü§ñ Initializing LLM service')
    const llmService = getLLMService()

    // Check available providers
    const providers = llmService.getAvailableProviders()
    console.log('üîå Available LLM providers:', providers)
    
    if (!providers.gemini && !providers.openai) {
      console.error('‚ùå No LLM providers configured')
      return NextResponse.json(
        { error: 'No LLM providers configured. Please set GOOGLE_API_KEY or OPENAI_API_KEY environment variables.' },
        { status: 500 }
      )
    }

    // Convert database messages to LLM format
    const chatMessages: ChatMessage[] = messages.map((msg: any) => ({
      role: msg.role,
      content: msg.content,
      fileAttachments: msg.fileAttachments || [],
    }))

    console.log('üí¨ Chat messages prepared:', chatMessages.length)

    // Generate completion with function calling disabled for now
    console.log('üéØ Generating LLM completion...')
    let response = await llmService.generateChatCompletion(chatMessages, {
      maxTokens: options?.maxTokens || 4000,
      temperature: options?.temperature || 0.7,
      systemPrompt: AGRICULTURAL_SYSTEM_PROMPT,
      enableFunctions: false, // Disabled for now - will use data source selector instead
    })

    console.log('üéØ LLM response received:', {
      model: response.model,
      contentLength: response.content?.length,
      hasFunctionCalls: !!response.functionCalls?.length,
      functionCallCount: response.functionCalls?.length || 0
    })

    // Handle function calls if present
    if (response.functionCalls && response.functionCalls.length > 0) {
      console.log('üîß Function calls detected:', response.functionCalls.map(fc => fc.name))
      
      // Execute all function calls
      console.log('‚ö° Executing function calls...')
      const functionResults = await Promise.all(
        response.functionCalls.map(async (functionCall, index) => {
          console.log(`üîß Executing function ${index + 1}/${response.functionCalls!.length}: ${functionCall.name}`)
          const result = await executeJohnDeereFunction(functionCall)
          return {
            name: functionCall.name,
            result,
            error: result.error ? result.error : undefined
          }
        })
      )

      console.log('‚úÖ All function calls completed:', functionResults.map(fr => ({ name: fr.name, hasError: !!fr.error })))

      // Add function results to conversation and get final response
      const messagesWithFunctions: ChatMessage[] = [
        ...chatMessages,
        {
          role: 'assistant',
          content: response.content,
          functionCall: response.functionCalls[0], // For simplicity, use first function call
        },
        ...functionResults.map(result => ({
          role: 'function' as const,
          content: JSON.stringify(result.result),
          functionResult: result,
        }))
      ]

      console.log('üéØ Getting final response with function results...')
      // Get final response with function results
      response = await llmService.generateChatCompletion(messagesWithFunctions, {
        maxTokens: options?.maxTokens || 4000,
        temperature: options?.temperature || 0.7,
        systemPrompt: AGRICULTURAL_SYSTEM_PROMPT,
        enableFunctions: false, // Disable functions for final response
      })

      console.log('‚úÖ Final response generated:', {
        model: response.model,
        contentLength: response.content?.length
      })
    }

    // Save assistant message to database
    console.log('üíæ Saving message to database...')
    const assistantMessage = await prisma.message.create({
      data: {
        sessionId: sessionId,
        role: 'assistant',
        content: response.content,
        metadata: {
          model: response.model,
          usage: response.usage,
          functionCalls: response.functionCalls ? JSON.parse(JSON.stringify(response.functionCalls)) : [],
        },
      },
    })

    console.log('‚úÖ Chat completion successful:', assistantMessage.id)

    return NextResponse.json({
      message: {
        id: assistantMessage.id,
        role: assistantMessage.role,
        content: assistantMessage.content,
        createdAt: assistantMessage.createdAt,
        metadata: assistantMessage.metadata,
      },
      usage: response.usage,
      model: response.model,
    })
  } catch (error) {
    console.error('‚ùå Error generating chat completion:', error)
    
    // Return specific error messages for common issues
    if (error instanceof Error) {
      console.error('‚ùå Error details:', {
        message: error.message,
        stack: error.stack,
        name: error.name
      })
      
      if (error.message.includes('API key')) {
        return NextResponse.json(
          { error: 'LLM API configuration error. Please check your API keys.' },
          { status: 500 }
        )
      }
      if (error.message.includes('quota') || error.message.includes('rate limit')) {
        return NextResponse.json(
          { error: 'API rate limit exceeded. Please try again later.' },
          { status: 429 }
        )
      }
    }

    return NextResponse.json(
      { error: 'Failed to generate response' },
      { status: 500 }
    )
  }
}

// GET endpoint to check LLM service status
export async function GET(request: NextRequest) {
  try {
    const llmService = getLLMService()
    const providers = llmService.getAvailableProviders()
    const config = llmService.getConfig()

    return NextResponse.json({
      status: 'ok',
      providers,
      config,
    })
  } catch (error) {
    console.error('Error checking LLM service status:', error)
    return NextResponse.json(
      { error: 'Failed to check LLM service status' },
      { status: 500 }
    )
  }
} 